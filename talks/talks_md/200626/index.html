<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo 0.81.0" />
  <link rel="stylesheet" href="https://ai-seminars.github.io/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  
  
  
  <link rel="stylesheet" href="https://ai-seminars.github.io/css/cayman.91340ce421a4d303dd421832d1825889f5d1df49d562e5704bf09163ae6e7c45.css">
  
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  
  <title>June 26th, 2020 Pawel Swietojanski (Scientia Fellow @CSE) | AI Seminars</title>
</head>

<body>
  <section class="page-header">
  <h1 class="project-name">
    AI Seminar Series
  </h1>
  <h2 class="project-tagline">
    Hosted by UNSW Computing
  </h2>
  <nav>
    
    
      
      
      
      
      <a href="/home" class="btn">Up Coming Talks</a>
    
      
      
      
      
      <a href="/schedule" class="btn">Schedule</a>
    
      
      
      
      
      <a href="/talks" class="btn">Previous Talks</a>
    
      
      
      
      
      <a href="/about" class="btn">About</a>
    
  </nav>
</section>

  <section class="main-content">
    
  <h1>June 26th, 2020 Pawel Swietojanski (Scientia Fellow @CSE)</h1>
  <h1 id="static-visual-spatial-priors-for-speech-processing">Static Visual Spatial Priors for Speech Processing</h1>
<h1 id="abstract">Abstract</h1>
<p>Human perception relies on multi-modal processing integrating many sensory inputs and auxiliary knowledge sources. In the particular case of hearing, research has shown that under high acoustic uncertainty (noisy / reverberant acoustic environments, unfamiliar speakers, simultaneous speech, etc.), our brains can efficiently leverage auxiliary sources such as vision and/or higher level conceptual understanding / common sense reasoning to better separate target speech sources or to fill inaudible gaps. It is thus interesting to consider similar mechanisms when designing machine perception algorithms. In this talk I will focus on speech processing, and on one special case of visual information we call static visual spatial prior. Such priors can be estimated infrequently, independently and asynchronously from the primary audio stream. This allows to relax compute-related requirements for parallel processing allowing for edge deployments which in turn offer similar level of privacy as one would get with sole audio stream. We show its efficacy on two benchmarks, one measures the accuracy of Direction of Arrival of Sound estimation (DoA). DoA is a basic building block in many applications that relies on microphone arrays to capture spatial sound. Another benchmark builds on top of DoA and shows how such priors with additional semantic information can improve acoustic modelling for distant speech recognition in ambiguous acoustic environments.</p>
<h1 id="bio">Bio</h1>
<p>Pawel Swietojanski is currently Scientia Fellow and Lecturer in the School of Computer Science and Engineering at UNSW Sydney. His main research interests include machine learning and its applications in spoken language processing. Prior to UNSW Pawel held research related positions in both industry and academia. Pawel got awarded PhD in Computer Science from University of Edinburgh in 2016.</p>


    <footer class="site-footer">
  <span class="site-footer-credits">
    Wafa Johal 2021.
  </span>
</footer>

  </section>
  
  

</body>
</html>
